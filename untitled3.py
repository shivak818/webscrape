# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bTXE66YiZOdCocPdJGMoawu4co7SmRG6
"""

!pip install selenium

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

!apt-get update
!apt-get install chromium-driver

def web_driver():
  options = webdriver.ChromeOptions()
  options.add_argument('--headless')
  options.add_argument('--verbose')
  options.add_argument('--no-sandbox')
  options.add_argument('--disable-dev-shm-usage')
  options.add_argument('--disable-gpu')
  driver = webdriver.Chrome(options=options)
  return driver

def scrape_projects(driver):
    url = "https://hprera.nic.in/PublicDashboard"
    driver.get(url)

    wait = WebDriverWait(driver, 100)

    # Click "Registered Projects" tab
    registered_projects_heading = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "a.nav-link.active[data-toggle='tab'][data-target='#reg-Projects']")))
    registered_projects_heading.click()

    project_details = []

    # CSS selectors for the first 6 projects
    project_selectors = [
        "#reg-Projects > div > div > div:nth-child(1) > div > div > a",
        "#reg-Projects > div > div > div:nth-child(2) > div > div > a",
        "#reg-Projects > div > div > div:nth-child(3) > div > div > a",
        "#reg-Projects > div > div > div:nth-child(4) > div > div > a",
        "#reg-Projects > div > div > div:nth-child(5) > div > div > a",
        "#reg-Projects > div > div > div:nth-child(6) > div > div > a"
    ]

    for selector in project_selectors:
        project_link = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, selector)))
        project_link.click()

        time.sleep(20)  # Wait for the details page to load

        # Extract project details
        name = driver.find_element(By.CSS_SELECTOR, "#project-menu-html > div:nth-child(2) > div:nth-child(1) > div > table > tbody > tr:nth-child(1) > td.fw-600").text
        pan_no = driver.find_element(By.CSS_SELECTOR, "#project-menu-html > div:nth-child(2) > div:nth-child(1) > div > table > tbody > tr:nth-child(6) > td:nth-child(2) > span").text
        gstin_no = driver.find_element(By.CSS_SELECTOR, "#project-menu-html > div:nth-child(2) > div:nth-child(1) > div > table > tbody > tr:nth-child(13) > td:nth-child(2) > span").text
        address = driver.find_element(By.CSS_SELECTOR, "#project-menu-html > div:nth-child(2) > div:nth-child(1) > div > table > tbody > tr:nth-child(12) > td:nth-child(2) > span").text

        project_info = {
            "GST No.": gstin_no,
            "PAN No.": pan_no,
            "Name": name,
            "Address": address
        }
        project_details.append(project_info)

        # Close the details popup
        close_button = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Close')]")))
        close_button.click()

        time.sleep(1)

    return project_details


# Call the scrape_projects function
scraped_projects = scrape_projects(driver)

# Print the scraped data
print(scraped_projects)

# Write the scraped data to CSV
csv_filename = "scraped_projects.csv"
with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
    fieldnames = ["Name", "Address", "GST No.", "PAN No."]
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for project in scraped_projects:
        writer.writerow(project)



print(f"Scraped data has been saved to '{csv_filename}'.")